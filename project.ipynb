{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd2b25d5",
   "metadata": {},
   "source": [
    "# Thesis Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157ff5a7",
   "metadata": {},
   "source": [
    "## Requirements and Imports\n",
    "\n",
    "All requirements in this project are managed by `Pipenv` and are denoted in the `Pipfile`, which can be found in the same directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b4b0800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import glob\n",
    "import wfdb\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "\n",
    "# PyTorch related imports\n",
    "import torch\n",
    "import torchinfo\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# RayTune related imports\n",
    "from ray import tune, put, get\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "# Sklearn and Scipy related imports\n",
    "from scipy import signal\n",
    "from statsmodels.tsa.stattools import ccf\n",
    "from skimage.restoration import denoise_wavelet\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8cb7b9",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "This the following code cell can be used to configure certain aspects of the experiment.\n",
    "To specify offsets (for instance for the sampling rate or the segment size), please refer to the official [pandas documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c145768a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a random seed for pytorch (always split the data the same way), for the random package and for numpy. Required for reproducability.\n",
    "random.seed(1658497162847124986)\n",
    "torch.manual_seed(1658497162847124986)\n",
    "np.random.seed(1658497162)\n",
    "\n",
    "# If a GPU (Cuda) is available, it will be used to train and test the machine learning models.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# The directory for saving and loading the trained models.\n",
    "model_directory = './models/'\n",
    "\n",
    "# Here, some basic settings for the datasets and are set.\n",
    "data_directory = './data/'\n",
    "sampling_rate = '10L' # 10ms, The signal will be resampled based on this sampling rate.\n",
    "segment_size = '10240L' # 10240 ms, The signal will be split based on this length.\n",
    "segment_size_samples = int(pd.to_timedelta(segment_size).total_seconds() / pd.to_timedelta(sampling_rate).total_seconds())\n",
    "train_split = 0.8\n",
    "\n",
    "# The target noise SNR values for training and testing.\n",
    "train_target_snr_dbs = [-1.5, 0, 1.5, 8.5, 10, 11.5] # dB\n",
    "test_target_snr_dbs =  [-1, 0, 0.5, 1, 3, 5, 8] # dB\n",
    "\n",
    "# Number of epochs for training.\n",
    "num_epochs = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbeb9e6",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "This is a utilitarian class which is used for managing the required data. It inherits from the PyTorch `Dataset` class, which provides tools like splitting the data into train and test subsets or using it in dataloaders to efficiently access the data. The dataset for this experiment coprises the CEBS dataset and another dataset, called D2. The data from the denoted datasets will be loaded directly when initialising an object of the class. Each dataset will be imported, depending on its specific characteristics, the schemas will be normalized, the two dataset will be resampled and each dataset will be grouped according to `segment_size_samples`. Subsequelty, the two datasets will be combined. Initially, no noise will be added to the data, i.e. the clean and noisy data will be the same in the beginning. This is done so that training and testing can be done with different noise levels. The noise generated is gaussian white noise. However, this can be substituted with some other noise generation mechanism later, when required. Please note that the code for generating the noise was adopted from [this](https://stackoverflow.com/a/53688043) example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46e9a7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(clean, noise):\n",
    "    x_concat = np.concatenate((clean, noise))\n",
    "    min_val = np.min(x_concat)\n",
    "    max_val = np.max(x_concat)\n",
    "    y_clean = (clean-min_val) / (max_val - min_val)\n",
    "    y_noise = (noise-min_val) / (max_val - min_val)\n",
    "    return y_clean, y_noise, min_val, max_val\n",
    "\n",
    "def denormalize(clean, noise, min_val, max_val):\n",
    "    y_clean = max_val * clean - clean * min_val + min_val\n",
    "    y_noise = max_val * noise - noise * min_val + min_val\n",
    "    return y_clean, y_noise\n",
    "\n",
    "def bandpass(input_signal):\n",
    "    # https://www.youtube.com/watch?v=juYqcck_GfU\n",
    "    fs = 100.0 # frequency space\n",
    "    lowcut = 5.0 # Hz\n",
    "    highcut = 30.0 # Hz\n",
    "\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "\n",
    "    order = 4\n",
    "\n",
    "    b, a, = signal.butter(order, [low, high], 'bandpass', analog=False)\n",
    "    y = signal.filtfilt(b, a, input_signal, axis=0)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9c8d52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SCGData(Dataset):\n",
    "\n",
    "    def __init__(self, directory=None, datasets=None, transform=None):\n",
    "\n",
    "        self.directory = directory\n",
    "        self.datasets = datasets\n",
    "        self.transform = transform\n",
    "\n",
    "        data = self.__load_datasets()\n",
    "\n",
    "        # data = data.apply(lambda x: bandpass(x))\n",
    "        data = pd.DataFrame({'clean': data, 'noise': data})\n",
    "        \n",
    "        self.x_data = data.noise.values\n",
    "        self.y_data = data.clean.values\n",
    "\n",
    "        self.n_samples = data.shape[0]\n",
    "\n",
    "\n",
    "    def add_noise(self, target_snr_dbs):\n",
    "        self.x_data = np.array([self.__add_gaussian(inputs, target_snr_dbs) for inputs in self.y_data])\n",
    "\n",
    "\n",
    "    def __add_gaussian(self, data, target_snr_dbs):\n",
    "\n",
    "        # target_snr_db = random.choices(target_snr_dbs, weights=target_snr_weights, k=1)[0]\n",
    "        target_snr_db = random.choice(target_snr_dbs)\n",
    "        # target_snr_db = random.uniform(-1, 20) # sample from bigger range provides better results?\n",
    "        # target_snr_db = 1\n",
    "\n",
    "        data_watts = data ** 2\n",
    "        data_avg_watts = np.mean(data_watts)\n",
    "        data_avg_db = 10 * np.log10(data_avg_watts)\n",
    "\n",
    "        noise_avg_db = data_avg_db - target_snr_db\n",
    "        noise_avg_watts = 10 ** (noise_avg_db / 10)\n",
    "\n",
    "        mean_noise = 0\n",
    "        noise = np.random.normal(mean_noise, np.sqrt(noise_avg_watts), len(data_watts))\n",
    "\n",
    "        data_noisy = data + noise\n",
    "        return data_noisy\n",
    "\n",
    "\n",
    "    # Function to import the CEBS dataset\n",
    "    def __load_cebs(self, filename):\n",
    "        \n",
    "        # Using WFDB to import the files.\n",
    "        record = wfdb.rdsamp(filename)\n",
    "        data = record[0]\n",
    "        metadata = record[1]\n",
    "\n",
    "        frequency = metadata['fs']\n",
    "        column_names = metadata['sig_name']\n",
    "        length = metadata['sig_len']\n",
    "\n",
    "        data = pd.DataFrame(data, columns=column_names)\n",
    "        data.drop(['I', 'II', 'RESP'], axis=1, inplace=True)\n",
    "\n",
    "        # Create a datetime index, as it is not given in the dataset. Required for resampling and splitting the data.\n",
    "        frequency_string = str(int(1/frequency*1000000)) + 'U'\n",
    "        index = pd.date_range(start='1/1/1970', periods=length, freq=frequency_string)\n",
    "        data.set_index(index, inplace=True)\n",
    "\n",
    "        # Resample the data into the desired sampling rate.\n",
    "        data = data.resample(sampling_rate).mean().SCG\n",
    "\n",
    "        # Split the data into groups with segment_size_samples samples.\n",
    "        data = data.groupby((data.index - data.index[0]).total_seconds() * 1e3 // (segment_size_samples * 10)).agg(list)\n",
    "        data = data[data.apply(lambda x: len(x) == segment_size_samples)]\n",
    "        data = data.apply(lambda x: np.array(x))\n",
    "        \n",
    "        return data\n",
    "\n",
    "\n",
    "    # Function to import the D2 dataset\n",
    "    def __load_d2(self, filename):\n",
    "\n",
    "        # Since the files are not structured the same, they have to be imported differently. The following code does that.\n",
    "        # Regex to identify files with specific names, as they are structured differently.\n",
    "        regex_up_1 = re.compile(r'./data/D2/UP-(((0|1)[0-9])|(20|21))-*') # Files named UP-[01-21]-Raw.csv\n",
    "        regex_up_2 = re.compile(r'./data/D2/UP-(22|23)-*') # Files named UP-[22-23]-Raw.csv\n",
    "\n",
    "        # Import and normalize the structures of the different files.\n",
    "        if re.match(r'./data/D2/CP-*', filename):\n",
    "            data = pd.read_csv(filename, \n",
    "                    sep=',', \n",
    "                    header=1, \n",
    "                    skiprows=[2],\n",
    "                    usecols=['Shimmer_D0CD_Timestamp_Shimmer_CAL', 'Shimmer_D0CD_Accel_LN_Z_CAL'], \n",
    "                    dtype={'Shimmer_D0CD_Timestamp_Shimmer_CAL': 'float', 'Shimmer_D0CD_Accel_LN_Z_CAL': 'float'})\n",
    "            data.rename(columns={'Shimmer_D0CD_Timestamp_Shimmer_CAL': 'index', 'Shimmer_D0CD_Accel_LN_Z_CAL': 'SCG'}, inplace=True)\n",
    "        elif regex_up_1.match(filename):\n",
    "            data = pd.read_csv(filename, \n",
    "                    sep='\t', \n",
    "                    header=1, \n",
    "                    skiprows=[2],\n",
    "                    usecols=['ECG_TimestampSync_Unix_CAL', 'ECG_Accel_LN_Z_CAL'], \n",
    "                    dtype={'ECG_TimestampSync_Unix_CAL': 'float', 'ECG_Accel_LN_Z_CAL': 'float'})\n",
    "            data.rename(columns={'ECG_TimestampSync_Unix_CAL': 'index', 'ECG_Accel_LN_Z_CAL': 'SCG'}, inplace=True)\n",
    "        elif regex_up_2.match(filename):\n",
    "            data = pd.read_csv(filename, \n",
    "                    sep=',', \n",
    "                    header=1, \n",
    "                    skiprows=[2],\n",
    "                    usecols=['ECG_Timestamp_Unix_CAL', 'ECG_Accel_LN_Z_CAL'], \n",
    "                    dtype={'ECG_Timestamp_Unix_CAL': 'float', 'ECG_Accel_LN_Z_CAL': 'float'})\n",
    "            data.rename(columns={'ECG_Timestamp_Unix_CAL': 'index', 'ECG_Accel_LN_Z_CAL': 'SCG'}, inplace=True)\n",
    "        elif re.match(r'./data/D2/UP-*', filename):\n",
    "            data = pd.read_csv(filename, \n",
    "                    sep=',', \n",
    "                    header=0, \n",
    "                    skiprows=[1],\n",
    "                    usecols=['ECG_Accel_LN_Z_CAL'], \n",
    "                    dtype={'ECG_Accel_LN_Z_CAL': 'float'})\n",
    "            data.rename(columns={'ECG_Accel_LN_Z_CAL': 'SCG'}, inplace=True)\n",
    "\n",
    "            # The datetime index is not set correctly. Therefore, it has to be set set manually. The frequency was obtained from the original paper. Please refer to the thesis for further information.\n",
    "            frequency = 512\n",
    "            frequency_string = str(int(1/frequency*1000000000)) + 'N'\n",
    "            data['index'] = pd.date_range(start='1/1/1970', periods=data.shape[0], freq=frequency_string)\n",
    "        else:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        data['index'] = pd.to_datetime(data['index'], unit='ms')\n",
    "        data.set_index('index', inplace=True)\n",
    "\n",
    "        # Resample the data into the desired sampling rate.\n",
    "        data = data.resample(sampling_rate).mean().SCG\n",
    "\n",
    "        # Split the data into groups with segment_size_samples samples.\n",
    "        data = data.groupby((data.index - data.index[0]).total_seconds() * 1e3 // (segment_size_samples * 10)).agg(list)\n",
    "        data = data[data.apply(lambda x: len(x) == segment_size_samples and not np.isnan(x).any())]\n",
    "        data = data.apply(lambda x: np.array(x))\n",
    "\n",
    "        return data\n",
    "\n",
    "    # Loads the datasets denoted in the datasets parameter.\n",
    "    def __load_datasets(self):\n",
    "\n",
    "        data = pd.Series(dtype='object')\n",
    "\n",
    "        # Loads the CEBS dataset from the 'CEBS' directory in the directory folder.\n",
    "        if 'CEBS' in self.datasets:\n",
    "            directory = os.path.join(self.directory, 'CEBS')\n",
    "            filenames = list(dict.fromkeys([x[:-4] for x in glob.glob(f'{directory}/*[0-9][0-9][0-9].*')]))\n",
    "            data = pd.concat((self.__load_cebs(filename) for filename in filenames), ignore_index=True)\n",
    "\n",
    "        # Loads the D2 dataset from the 'D2' directory in the directory folder.\n",
    "        if 'D2' in self.datasets:\n",
    "            directory = os.path.join(self.directory, 'D2')\n",
    "            filenames = glob.glob(f'{directory}/*')\n",
    "            data2 = pd.concat((self.__load_d2(filename) for filename in filenames), ignore_index=True)\n",
    "            data = pd.concat((data, data2))\n",
    "\n",
    "        return data\n",
    "\n",
    "    # Returns a sample with the specified index and applies the transformation, if it is set.\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.x_data[index].copy(), self.y_data[index].copy()\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        \n",
    "        return sample\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "\n",
    "# Transformation class used to transform the numpy arrays to PyTorch tensors.\n",
    "class ToTensor:\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        inputs, targets = sample\n",
    "        return torch.Tensor(inputs), torch.Tensor(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1492b678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the data and returns splitted test and train datasets\n",
    "def load_data(directory, datasets, transform):\n",
    "    dataset = SCGData(directory=directory, datasets=datasets, transform=transform)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    return torch.utils.data.random_split(dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97834ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports the dataset and splits it into a train and test subset. \n",
    "train_dataset, test_dataset = load_data(directory=data_directory, datasets=['CEBS', 'D2'], transform=ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b837dfa1",
   "metadata": {},
   "source": [
    "## Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba01cc41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Input Shape               Output Shape\n",
       "==========================================================================================\n",
       "DeNoise1                                 [32, 1, 1024]             [32, 1, 1024]\n",
       "├─Sequential: 1-1                        [32, 1, 1024]             [32, 1, 1000]\n",
       "│    └─Conv1d: 2-1                       [32, 1, 1024]             [32, 128, 1018]\n",
       "│    └─BatchNorm1d: 2-2                  [32, 128, 1018]           [32, 128, 1018]\n",
       "│    └─ELU: 2-3                          [32, 128, 1018]           [32, 128, 1018]\n",
       "│    └─Conv1d: 2-4                       [32, 128, 1018]           [32, 64, 1012]\n",
       "│    └─BatchNorm1d: 2-5                  [32, 64, 1012]            [32, 64, 1012]\n",
       "│    └─ELU: 2-6                          [32, 64, 1012]            [32, 64, 1012]\n",
       "│    └─Conv1d: 2-7                       [32, 64, 1012]            [32, 32, 1006]\n",
       "│    └─BatchNorm1d: 2-8                  [32, 32, 1006]            [32, 32, 1006]\n",
       "│    └─ELU: 2-9                          [32, 32, 1006]            [32, 32, 1006]\n",
       "│    └─Conv1d: 2-10                      [32, 32, 1006]            [32, 1, 1000]\n",
       "│    └─BatchNorm1d: 2-11                 [32, 1, 1000]             [32, 1, 1000]\n",
       "│    └─ELU: 2-12                         [32, 1, 1000]             [32, 1, 1000]\n",
       "├─Sequential: 1-2                        [32, 1, 1000]             [32, 1, 1024]\n",
       "│    └─ConvTranspose1d: 2-13             [32, 1, 1000]             [32, 32, 1006]\n",
       "│    └─BatchNorm1d: 2-14                 [32, 32, 1006]            [32, 32, 1006]\n",
       "│    └─ELU: 2-15                         [32, 32, 1006]            [32, 32, 1006]\n",
       "│    └─ConvTranspose1d: 2-16             [32, 32, 1006]            [32, 64, 1012]\n",
       "│    └─BatchNorm1d: 2-17                 [32, 64, 1012]            [32, 64, 1012]\n",
       "│    └─ELU: 2-18                         [32, 64, 1012]            [32, 64, 1012]\n",
       "│    └─ConvTranspose1d: 2-19             [32, 64, 1012]            [32, 128, 1018]\n",
       "│    └─BatchNorm1d: 2-20                 [32, 128, 1018]           [32, 128, 1018]\n",
       "│    └─ELU: 2-21                         [32, 128, 1018]           [32, 128, 1018]\n",
       "│    └─ConvTranspose1d: 2-22             [32, 128, 1018]           [32, 1, 1024]\n",
       "│    └─BatchNorm1d: 2-23                 [32, 1, 1024]             [32, 1, 1024]\n",
       "│    └─ELU: 2-24                         [32, 1, 1024]             [32, 1, 1024]\n",
       "│    └─Conv1d: 2-25                      [32, 1, 1024]             [32, 1, 1024]\n",
       "│    └─ReLU: 2-26                        [32, 1, 1024]             [32, 1, 1024]\n",
       "==========================================================================================\n",
       "Total params: 146,958\n",
       "Trainable params: 146,958\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 4.74\n",
       "==========================================================================================\n",
       "Input size (MB): 0.13\n",
       "Forward/backward pass size (MB): 234.02\n",
       "Params size (MB): 0.59\n",
       "Estimated Total Size (MB): 234.74\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DeNoise1(nn.Module):\n",
    "    def __init__(self, layer_size=128):\n",
    "        super(DeNoise1, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(1, layer_size, 7, stride=1),\n",
    "            nn.BatchNorm1d(layer_size),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(layer_size, int(layer_size/2), 7, stride=1),\n",
    "            nn.BatchNorm1d(int(layer_size/2)),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(int(layer_size/2), int(layer_size/4), 7, stride=1),\n",
    "            nn.BatchNorm1d(int(layer_size/4)),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(int(layer_size/4), 1, 7, stride=1),\n",
    "            nn.BatchNorm1d(1),\n",
    "            nn.ELU()\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose1d(1, int(layer_size/4), 7, stride=1),\n",
    "            nn.BatchNorm1d(int(layer_size/4)),\n",
    "            nn.ELU(),\n",
    "            nn.ConvTranspose1d(int(layer_size/4), int(layer_size/2), 7, stride=1),\n",
    "            nn.BatchNorm1d(int(layer_size/2)),\n",
    "            nn.ELU(),\n",
    "            nn.ConvTranspose1d(int(layer_size/2), layer_size, 7, stride=1),\n",
    "            nn.BatchNorm1d(layer_size),\n",
    "            nn.ELU(),\n",
    "            nn.ConvTranspose1d(layer_size, 1, 7, stride=1), \n",
    "            nn.BatchNorm1d(1),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(1, 1, 7, stride=1, padding='same'),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "# Prints the structure of the Autoencoder\n",
    "torchinfo.summary(DeNoise1(), (32, 1, segment_size_samples), col_names=('input_size', 'output_size'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "031c1a44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Input Shape               Output Shape\n",
       "==========================================================================================\n",
       "DeNoise2                                 [32, 1, 1024]             [32, 1, 1024]\n",
       "├─Sequential: 1-1                        [32, 1, 1024]             [32, 1, 497]\n",
       "│    └─Conv1d: 2-1                       [32, 1, 1024]             [32, 128, 1018]\n",
       "│    └─BatchNorm1d: 2-2                  [32, 128, 1018]           [32, 128, 1018]\n",
       "│    └─ELU: 2-3                          [32, 128, 1018]           [32, 128, 1018]\n",
       "│    └─Conv1d: 2-4                       [32, 128, 1018]           [32, 32, 1012]\n",
       "│    └─BatchNorm1d: 2-5                  [32, 32, 1012]            [32, 32, 1012]\n",
       "│    └─ELU: 2-6                          [32, 32, 1012]            [32, 32, 1012]\n",
       "│    └─Conv1d: 2-7                       [32, 32, 1012]            [32, 16, 503]\n",
       "│    └─BatchNorm1d: 2-8                  [32, 16, 503]             [32, 16, 503]\n",
       "│    └─ELU: 2-9                          [32, 16, 503]             [32, 16, 503]\n",
       "│    └─Conv1d: 2-10                      [32, 16, 503]             [32, 1, 497]\n",
       "│    └─BatchNorm1d: 2-11                 [32, 1, 497]              [32, 1, 497]\n",
       "│    └─ELU: 2-12                         [32, 1, 497]              [32, 1, 497]\n",
       "├─Sequential: 1-2                        [32, 1, 497]              [32, 1, 1024]\n",
       "│    └─ConvTranspose1d: 2-13             [32, 1, 497]              [32, 16, 503]\n",
       "│    └─BatchNorm1d: 2-14                 [32, 16, 503]             [32, 16, 503]\n",
       "│    └─ELU: 2-15                         [32, 16, 503]             [32, 16, 503]\n",
       "│    └─ConvTranspose1d: 2-16             [32, 16, 503]             [32, 32, 1012]\n",
       "│    └─BatchNorm1d: 2-17                 [32, 32, 1012]            [32, 32, 1012]\n",
       "│    └─ELU: 2-18                         [32, 32, 1012]            [32, 32, 1012]\n",
       "│    └─ConvTranspose1d: 2-19             [32, 32, 1012]            [32, 128, 1018]\n",
       "│    └─BatchNorm1d: 2-20                 [32, 128, 1018]           [32, 128, 1018]\n",
       "│    └─ELU: 2-21                         [32, 128, 1018]           [32, 128, 1018]\n",
       "│    └─ConvTranspose1d: 2-22             [32, 128, 1018]           [32, 1, 1024]\n",
       "│    └─BatchNorm1d: 2-23                 [32, 1, 1024]             [32, 1, 1024]\n",
       "│    └─ELU: 2-24                         [32, 1, 1024]             [32, 1, 1024]\n",
       "│    └─Conv1d: 2-25                      [32, 1, 1024]             [32, 1, 1024]\n",
       "│    └─ReLU: 2-26                        [32, 1, 1024]             [32, 1, 1024]\n",
       "==========================================================================================\n",
       "Total params: 67,598\n",
       "Trainable params: 67,598\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 2.11\n",
       "==========================================================================================\n",
       "Input size (MB): 0.13\n",
       "Forward/backward pass size (MB): 175.87\n",
       "Params size (MB): 0.27\n",
       "Estimated Total Size (MB): 176.28\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DeNoise2(nn.Module):\n",
    "    def __init__(self, layer_size=128):\n",
    "        super(DeNoise2, self).__init__()\n",
    "\n",
    "        # (i-k +2*p)/s+1\n",
    "        # (i-1)*s+k-2p\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(1, layer_size, 7, stride=1, padding=0),\n",
    "            nn.BatchNorm1d(layer_size),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(layer_size, int(layer_size/4), 7, stride=1, padding=0),\n",
    "            nn.BatchNorm1d(int(layer_size/4)),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(int(layer_size/4), int(layer_size/8), 7, stride=2, padding=0),\n",
    "            nn.BatchNorm1d(int(layer_size/8)),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(int(layer_size/8), 1, 7, stride=1, padding=0),\n",
    "            nn.BatchNorm1d(1),\n",
    "            nn.ELU()\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose1d(1, int(layer_size/8), 7, stride=1, padding=0),\n",
    "            nn.BatchNorm1d(int(layer_size/8)),\n",
    "            nn.ELU(),\n",
    "            nn.ConvTranspose1d(int(layer_size/8), int(layer_size/4), 7, stride=2, output_padding=1),\n",
    "            nn.BatchNorm1d(int(layer_size/4)),\n",
    "            nn.ELU(),\n",
    "            nn.ConvTranspose1d(int(layer_size/4), int(layer_size), 7, stride=1, output_padding=0),\n",
    "            nn.BatchNorm1d(int(layer_size)),\n",
    "            nn.ELU(),\n",
    "            nn.ConvTranspose1d(int(layer_size), 1, 7, stride=1, padding=0), \n",
    "            nn.BatchNorm1d(1),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(1, 1, 7, stride=1, padding='same'),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "# Prints the structure of the Autoencoder\n",
    "torchinfo.summary(DeNoise2(), (32, 1, segment_size_samples), col_names=('input_size', 'output_size'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba01cc41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Input Shape               Output Shape\n",
       "==========================================================================================\n",
       "DeNoise3                                 [32, 1, 1024]             [32, 1, 1024]\n",
       "├─Sequential: 1-1                        [32, 1, 1024]             [32, 1, 5]\n",
       "│    └─Conv1d: 2-1                       [32, 1, 1024]             [32, 128, 505]\n",
       "│    └─BatchNorm1d: 2-2                  [32, 128, 505]            [32, 128, 505]\n",
       "│    └─ELU: 2-3                          [32, 128, 505]            [32, 128, 505]\n",
       "│    └─Conv1d: 2-4                       [32, 128, 505]            [32, 64, 246]\n",
       "│    └─BatchNorm1d: 2-5                  [32, 64, 246]             [32, 64, 246]\n",
       "│    └─ELU: 2-6                          [32, 64, 246]             [32, 64, 246]\n",
       "│    └─Conv1d: 2-7                       [32, 64, 246]             [32, 64, 117]\n",
       "│    └─BatchNorm1d: 2-8                  [32, 64, 117]             [32, 64, 117]\n",
       "│    └─ELU: 2-9                          [32, 64, 117]             [32, 64, 117]\n",
       "│    └─Conv1d: 2-10                      [32, 64, 117]             [32, 64, 52]\n",
       "│    └─BatchNorm1d: 2-11                 [32, 64, 52]              [32, 64, 52]\n",
       "│    └─ELU: 2-12                         [32, 64, 52]              [32, 64, 52]\n",
       "│    └─Conv1d: 2-13                      [32, 64, 52]              [32, 128, 20]\n",
       "│    └─BatchNorm1d: 2-14                 [32, 128, 20]             [32, 128, 20]\n",
       "│    └─ELU: 2-15                         [32, 128, 20]             [32, 128, 20]\n",
       "│    └─Conv1d: 2-16                      [32, 128, 20]             [32, 1, 5]\n",
       "│    └─BatchNorm1d: 2-17                 [32, 1, 5]                [32, 1, 5]\n",
       "│    └─ELU: 2-18                         [32, 1, 5]                [32, 1, 5]\n",
       "├─Sequential: 1-2                        [32, 1, 5]                [32, 1, 1024]\n",
       "│    └─ConvTranspose1d: 2-19             [32, 1, 5]                [32, 1, 20]\n",
       "│    └─BatchNorm1d: 2-20                 [32, 1, 20]               [32, 1, 20]\n",
       "│    └─ELU: 2-21                         [32, 1, 20]               [32, 1, 20]\n",
       "│    └─ConvTranspose1d: 2-22             [32, 1, 20]               [32, 128, 52]\n",
       "│    └─BatchNorm1d: 2-23                 [32, 128, 52]             [32, 128, 52]\n",
       "│    └─ELU: 2-24                         [32, 128, 52]             [32, 128, 52]\n",
       "│    └─ConvTranspose1d: 2-25             [32, 128, 52]             [32, 64, 118]\n",
       "│    └─BatchNorm1d: 2-26                 [32, 64, 118]             [32, 64, 118]\n",
       "│    └─ELU: 2-27                         [32, 64, 118]             [32, 64, 118]\n",
       "│    └─ConvTranspose1d: 2-28             [32, 64, 118]             [32, 64, 250]\n",
       "│    └─BatchNorm1d: 2-29                 [32, 64, 250]             [32, 64, 250]\n",
       "│    └─ELU: 2-30                         [32, 64, 250]             [32, 64, 250]\n",
       "│    └─ConvTranspose1d: 2-31             [32, 64, 250]             [32, 64, 514]\n",
       "│    └─BatchNorm1d: 2-32                 [32, 64, 514]             [32, 64, 514]\n",
       "│    └─ELU: 2-33                         [32, 64, 514]             [32, 64, 514]\n",
       "│    └─ConvTranspose1d: 2-34             [32, 64, 514]             [32, 128, 1039]\n",
       "│    └─BatchNorm1d: 2-35                 [32, 128, 1039]           [32, 128, 1039]\n",
       "│    └─ELU: 2-36                         [32, 128, 1039]           [32, 128, 1039]\n",
       "│    └─Conv1d: 2-37                      [32, 128, 1039]           [32, 1, 1024]\n",
       "│    └─ReLU: 2-38                        [32, 1, 1024]             [32, 1, 1024]\n",
       "==========================================================================================\n",
       "Total params: 797,335\n",
       "Trainable params: 797,335\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 8.04\n",
       "==========================================================================================\n",
       "Input size (MB): 0.13\n",
       "Forward/backward pass size (MB): 148.68\n",
       "Params size (MB): 3.19\n",
       "Estimated Total Size (MB): 152.00\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DeNoise3(nn.Module):\n",
    "    def __init__(self, kernel_size=16, filters_1=128, filters_2=64):\n",
    "        super(DeNoise3, self).__init__()\n",
    "        #self.p1 = segment_size_samples/2 + 7\n",
    "        #self.p2 = segment_size_samples/64 + 7\n",
    "        # (i-k +2*p)/s+1\n",
    "        # (i-1)*s+k-2p\n",
    "\n",
    "        self.kernel_size = kernel_size\n",
    "        self.filters_1 = filters_1\n",
    "        self.filters_2 = filters_2\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(1, filters_1, kernel_size, stride=2),\n",
    "            nn.BatchNorm1d(filters_1),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(filters_1, filters_2, kernel_size, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(filters_2),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(filters_2, filters_2, kernel_size, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(filters_2),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(filters_2, filters_2, kernel_size, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(filters_2),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(filters_2, filters_1, kernel_size, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(filters_1),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(filters_1, 1, kernel_size, stride=1),\n",
    "            nn.BatchNorm1d(1),\n",
    "            nn.ELU()\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose1d(1, 1, kernel_size, stride=1),\n",
    "            nn.BatchNorm1d(1),\n",
    "            nn.ELU(),\n",
    "            nn.ConvTranspose1d(1, filters_1, kernel_size, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(filters_1),\n",
    "            nn.ELU(),\n",
    "            nn.ConvTranspose1d(filters_1, filters_2, kernel_size, stride=2, padding=0),\n",
    "            nn.BatchNorm1d(filters_2),\n",
    "            nn.ELU(),\n",
    "            nn.ConvTranspose1d(filters_2, filters_2, kernel_size, stride=2, padding=0),\n",
    "            nn.BatchNorm1d(filters_2),\n",
    "            nn.ELU(),\n",
    "            nn.ConvTranspose1d(filters_2, filters_2, kernel_size, stride=2, padding=0),\n",
    "            nn.BatchNorm1d(filters_2),\n",
    "            nn.ELU(),\n",
    "            nn.ConvTranspose1d(filters_2, filters_1, kernel_size, stride=2, padding=2, output_padding=1),\n",
    "            nn.BatchNorm1d(filters_1),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(filters_1, 1, kernel_size, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = x.view(-1, 1, segment_size_samples)\n",
    "        encoded = self.encoder(x)\n",
    "        #encoded = encoded.view(-1, 1, segment_size_samples//16, 1)\n",
    "        #encoded = nn.functional.pad(encoded, (0, 0, 1, 0))\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "#print(segment_size_samples/2 + 7)\n",
    "#print(segment_size_samples/(2*8) + 7)\n",
    "torchinfo.summary(DeNoise3(), (32, 1, segment_size_samples), col_names=('input_size', 'output_size'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc39d04",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c7816ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config, train_dataset, model_num, is_tune=False, data_id=None, checkpoint_dir=None, data_dir=None):\n",
    "\n",
    "    if model_num in [1]:\n",
    "        model = DeNoise1(layer_size=config['layer_size'])\n",
    "    elif model_num in [2, 3]:\n",
    "        model = DeNoise2(layer_size=config['layer_size'])\n",
    "    elif model_num in [4]:\n",
    "        model = DeNoise3()\n",
    "    model = model.to(device)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], weight_decay=config['wd'])\n",
    "\n",
    "    # if checkpoint_dir != None:\n",
    "    #    model_state, optimizer_state = torch.load(os.path.join(checkpoint_dir, \"checkpoint\"))\n",
    "    #    model.load_state_dict(model_state)\n",
    "    #    optimizer.load_state_dict(optimizer_state)\n",
    "\n",
    "    if data_id:\n",
    "        train_dataset = get(data_id)\n",
    "\n",
    "    test_size = int(0.8 * len(train_dataset))\n",
    "    val_size = len(train_dataset) - test_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [test_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=config['batch_size'], shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(dataset=val_dataset, batch_size=config['batch_size'], shuffle=True, num_workers=2)\n",
    "\n",
    "    n_total_steps = len(train_loader)\n",
    "    min_val_loss = np.inf\n",
    "\n",
    "    train_loss_values = []\n",
    "    val_loss_values = []\n",
    "    epoch_values = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0.0\n",
    "        epoch_steps = 0\n",
    "        for i, (inputs, lables) in enumerate(train_loader):\n",
    "            \n",
    "            lables, inputs, _, _ = normalize(lables, inputs)\n",
    "\n",
    "            if model_num == 3:\n",
    "                inputs = denoise_wavelet(inputs, method='BayesShrink', mode='soft', wavelet_levels=1, wavelet='sym8', rescale_sigma='True')\n",
    "                \n",
    "            # inputs, lables = torch.from_numpy(inputs), torch.from_numpy(lables)\n",
    "            inputs, lables = inputs.view(-1, 1, segment_size_samples), lables.view(-1, 1, segment_size_samples)\n",
    "            inputs, lables = inputs.to(device), lables.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(inputs)\n",
    "            loss = criterion(output, lables)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            epoch_steps += 1\n",
    "\n",
    "        train_loss_values.append(train_loss)\n",
    "\n",
    "        val_loss = 0.0\n",
    "        val_steps = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for i, (inputs, lables) in enumerate(val_loader):\n",
    "\n",
    "            lables, inputs, _, _ = normalize(lables, inputs)\n",
    "\n",
    "            if model_num == 3:\n",
    "                inputs = denoise_wavelet(inputs, method='BayesShrink', mode='soft', wavelet_levels=1, wavelet='sym8', rescale_sigma='True')\n",
    "\n",
    "            # inputs, lables = torch.from_numpy(inputs), torch.from_numpy(lables)\n",
    "            inputs, lables = inputs.view(-1, 1, segment_size_samples), lables.view(-1, 1, segment_size_samples)\n",
    "            inputs, lables = inputs.to(device), lables.to(device)\n",
    "\n",
    "            output = model(inputs)\n",
    "            loss = criterion(output, lables)\n",
    "            val_loss += loss.item()\n",
    "            val_steps += 1\n",
    "            \n",
    "        val_loss_values.append(val_loss)\n",
    "        epoch_values.append(epoch)\n",
    "\n",
    "        if is_tune:\n",
    "            with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
    "                path = os.path.join(checkpoint_dir, 'checkpoint')\n",
    "                torch.save((model.state_dict(), optimizer.state_dict()), path)\n",
    "            \n",
    "            tune.report(loss=(val_loss / val_steps))\n",
    "        else:\n",
    "            print(f'epoch {epoch + 1} / {num_epochs}, train loss = {(train_loss / len(train_loader)):.10f}, val loss = {(val_loss / len(val_loader)):.10f}')\n",
    "            pd.DataFrame({'epoch': epoch_values,'train_loss': train_loss_values, 'val_loss': val_loss_values}).to_csv(f'./models/loss_{model_num}.csv') \n",
    "            \n",
    "            if min_val_loss > val_loss:\n",
    "                \n",
    "                print(f'\\tval loss decreased from {min_val_loss:.10f} to {val_loss:.10f}')\n",
    "                min_val_loss = val_loss\n",
    "                torch.save(model.state_dict(), os.path.join(model_directory, f'model_{model_num}.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "354f4492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'lr': 0.0018729000345412963, 'wd': 0, 'batch_size': 8, 'layer_size': 256} 0.0004914292857526488\n",
    "# {'lr': 0.0018437162360595998, 'wd': 0, 'batch_size': 8, 'layer_size': 128} 0.0007439919008643882\n",
    "# {'lr': 0.0006948120282769452, 'wd': 0, 'batch_size': 16, 'layer_size': 128} 0.0013009571703150868\n",
    "\n",
    "# {'lr': 0.004531902539657156, 'wd': 0, 'batch_size': 8, 'layer_size': 256} 0.0005863353168515718 w/o bandpass, 10\n",
    "# {'lr': 0.0017668188704620857, 'wd': 0, 'batch_size': 8, 'layer_size': 256} 0.0009116247971238728 bandpass, 10\n",
    "# {'lr': 0.006820280279552952, 'wd': 0, 'batch_size': 8, 'layer_size': 256} 0.0004986349987892337 with 2x stride 2\n",
    "# {'lr': 0.007810417589096403, 'wd': 0, 'batch_size': 8, 'layer_size': 256} 0.0004332348386333898 with 1x stride 2\n",
    "# {'lr': 0.0015365439418637585, 'wd': 0, 'batch_size': 8, 'layer_size': 128} 0.0007714835297364866\n",
    "# {'lr': 0.02716233959851866, 'wd': 0, 'batch_size': 32, 'layer_size': 128} 0.0013283919510080145\n",
    "# {'lr': 0.0019890548242081614, 'wd': 0, 'batch_size': 16, 'layer_size': 128} 0.00144807931687008\n",
    "# {'lr': 0.0038625945811403732, 'wd': 0, 'batch_size': 16, 'layer_size': 128} 0.0013378030157384826\n",
    "# {'lr': 0.0017112891648358517, 'wd': 0, 'batch_size': 32, 'layer_size': 128} 0.0019708356504062456\n",
    "# {'lr': 0.0029673206198941445, 'wd': 0, 'batch_size': 8, 'layer_size': 128} 0.0008134577474093409\n",
    "# {'lr': 0.00032813453637735567, 'wd': 0, 'batch_size': 64, 'layer_size': 128} 0.0008\n",
    "# {'lr': 0.0011704653180570925, 'wd': 0, 'batch_size': 8, 'layer_size': 128} 0.0010375047129388723\n",
    "# {'lr': 0.0025766834278764987, 'wd': 0, 'batch_size': 16, 'layer_size': 128} 0.0018318614236014928\n",
    "\n",
    "# Add the correct noise levels for the training process.\n",
    "# train_dataset.dataset.add_noise(train_target_snr_dbs)\n",
    "\n",
    "# train({'lr': 0.002, 'wd': 0, 'batch_size': 8, 'layer_size': 256}, train_dataset, 1)\n",
    "# train({'lr': 0.007, 'wd': 0, 'batch_size': 8, 'layer_size': 256}, train_dataset, 2)\n",
    "# train({'lr': 0.0005, 'wd': 0, 'batch_size': 8, 'layer_size': 128}, train_dataset, 3)\n",
    "# train({'lr': 0.005, 'wd': 0, 'batch_size': 8, 'layer_size': 128}, train_dataset, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550d9c55",
   "metadata": {},
   "source": [
    "## Hyperparametertuning\n",
    "\n",
    "The following code cell implements the hyperparametertuning. It was modeled after a [tutorial](https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html) by PyTorch, using RayTune as a tool for hyperparameter tuning. For optimizing the hyperparameters, the `ASHAScheduler` is being used and the loss should be minimized. The configuration defines the hyperparameters to be optimized. Here only the learning rate `lr`, the batch size `batch_size` and the layer size `layer_size` of the models will be taken into consideration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd2406ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypertuning(config, num_samples, max_num_epochs, model_num):\n",
    "\n",
    "    train_dataset.dataset.add_noise(train_target_snr_dbs)\n",
    "    scheduler = ASHAScheduler(metric='loss', mode='min', max_t=max_num_epochs, grace_period=1, reduction_factor=2)\n",
    "    reporter = CLIReporter(metric_columns=['loss', 'training_iteration'])\n",
    "    data_id = put(train_dataset)\n",
    "    result = tune.run(\n",
    "        partial(train, train_dataset=None, model_num=model_num, is_tune=True, data_id=data_id, checkpoint_dir=None, data_dir=None),\n",
    "        resources_per_trial={'cpu': 4, 'gpu': 1},\n",
    "        config=config,\n",
    "        num_samples=num_samples,\n",
    "        scheduler=scheduler,\n",
    "        progress_reporter=reporter\n",
    "    )\n",
    "\n",
    "    best_trial = result.get_best_trial('loss', 'min', 'last')\n",
    "    print(\"Best trial config: {}\".format(best_trial.config))\n",
    "    print(\"Best trial final validation loss: {}\".format(best_trial.last_result[\"loss\"]))\n",
    "\n",
    "    best_trained_model =  DeNoise2().to(device)\n",
    "\n",
    "    best_checkpoint_dir = best_trial.checkpoint.dir_or_data\n",
    "    model_state, optimizer_state = torch.load(os.path.join(best_checkpoint_dir, 'checkpoint'))\n",
    "    best_trained_model.load_state_dict(model_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0cfe8d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'lr': tune.loguniform(1e-4, 1e-1),\n",
    "    'wd': 0,\n",
    "    'batch_size': tune.choice([8]),\n",
    "    'layer_size': tune.choice([256])\n",
    "}\n",
    "\n",
    "# hypertuning(config, num_samples=16, max_num_epochs=128, model_num=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21410168",
   "metadata": {},
   "source": [
    "## Testing and Evaluation\n",
    "\n",
    "The following code cells are used for testing and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d3bbad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, some evaluation metrics are implemented.\n",
    "\n",
    "# Mean-Squared Error\n",
    "def mse(signal_clean, signal_noisy):\n",
    "    return mean_squared_error(signal_clean, signal_noisy)\n",
    "\n",
    "# Mean-Absolute Error\n",
    "def mae(signal_clean, signal_noisy):\n",
    "    return mean_absolute_error(signal_clean, signal_noisy)\n",
    "\n",
    "# Root Mean-Squared Error\n",
    "def rmse(signal_clean, signal_noisy):\n",
    "    return np.sqrt(mse(signal_clean, signal_noisy))\n",
    "\n",
    "# Signal-to-noise ratio\n",
    "def snr(signal_clean, signal_noisy):\n",
    "    return 10 * np.log10(np.sum(signal_clean ** 2) / (np.sum((signal_noisy - signal_clean) ** 2)))\n",
    "\n",
    "# Peak signal-to-noise ratio\n",
    "def psnr(signal_clean, signal_noisy):\n",
    "    max_clean = np.max(signal_clean)\n",
    "    rmse_val = rmse(signal_clean, signal_noisy)\n",
    "    return 20 * np.log10(max_clean / rmse_val)\n",
    "\n",
    "# Root mean squared difference\n",
    "def prd(signal_clean, signal_noisy):\n",
    "    return np.sqrt(np.sum((signal_clean - signal_noisy) ** 2) / np.sum(signal_clean ** 2)) * 100\n",
    "\n",
    "# Cross-Correlation https://stackoverflow.com/a/53436325\n",
    "def xcorr(signal_clean, signal_noisy):\n",
    "    return np.mean(ccf(signal_clean, signal_noisy, adjusted=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dfd42bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_result_bar(name_short, results, labels, target_snr_print, xlabel, ylabel, title, directory):\n",
    "    data = [[np.mean(results[method][scope][name_short]) for method in results.keys()] for scope in target_snr_print + ['all']]\n",
    "    plt.xticks(range(len(data[0])), labels)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    width = 0.12\n",
    "\n",
    "    for i in range(len(target_snr_print) + 1):\n",
    "        plt.bar(np.arange(len(data[i])) + i * width, data[i], width=width)\n",
    "    plt.legend(target_snr_print + ['all'])\n",
    "    \n",
    "    plt.savefig(os.path.join(directory, name_short + '.png'), format='png')\n",
    "    plt.show()\n",
    "\n",
    "def save_results(results, config):\n",
    "\n",
    "    if not os.path.exists('./results'):\n",
    "        os.mkdir('./results')\n",
    "\n",
    "    # run_name = str(len(os.listdir('./results')))\n",
    "    # working_dir = os.path.join('./results', run_name)\n",
    "    # working_dir = os.mkdir(working_dir)\n",
    "\n",
    "    working_dir = os.path.join('./results', '2')\n",
    "\n",
    "    results_print = {method: {scope: {evaluation: float(np.mean(values)) for evaluation, values in eval_method.items()} for scope, eval_method in method_value.items()} for method, method_value in results.items()}\n",
    "    with open(os.path.join(working_dir, 'results.json'), 'w+') as output_file:\n",
    "        json.dump(results_print, output_file, indent=4)\n",
    "\n",
    "    plot_result_bar('snr_imp', results, config['labels'], config['target_snr_print'], config['xlabel'], 'SNR Improvement (dB)', 'Mean SNR Improvement', working_dir)\n",
    "    plot_result_bar('mse', results, config['labels'], config['target_snr_print'], config['xlabel'], 'MSE', 'Mean MSE', working_dir)\n",
    "    plot_result_bar('rmse', results, config['labels'], config['target_snr_print'], config['xlabel'], 'RMSE', 'Mean RMSE', working_dir)\n",
    "    plot_result_bar('mae', results, config['labels'], config['target_snr_print'], config['xlabel'], 'MAE', 'Mean MAE', working_dir)\n",
    "    plot_result_bar('psnr', results, config['labels'], config['target_snr_print'], config['xlabel'], 'PSNR', 'Mean PSNR', working_dir)\n",
    "    plot_result_bar('xcorr', results, config['labels'], config['target_snr_print'], config['xlabel'], 'Cross Correlation', 'Mean Cross Correlation', working_dir)\n",
    "    plot_result_bar('prd', results, config['labels'], config['target_snr_print'], config['xlabel'], 'PRD (%)', 'Mean PRD', working_dir)\n",
    "\n",
    "    # CPU Consumption\n",
    "    data = [np.mean(results[method]['all']['cpu_time']) * 1000 for method in results.keys()]\n",
    "\n",
    "    plt.xticks(range(len(data)), config['labels'])\n",
    "    plt.xlabel('Denoising Method')\n",
    "    plt.ylabel('CPU Time (ms)')\n",
    "    plt.title('Mean CPU Time')\n",
    "\n",
    "    plt.bar(np.arange(len(data)), data)\n",
    "    \n",
    "    plt.savefig(os.path.join(working_dir, 'cpu.png'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ab33828",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(config):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        ae1 = DeNoise2(layer_size=256).to(device)\n",
    "        ae1.load_state_dict(torch.load(os.path.join(model_directory, config['models'][0])))\n",
    "        ae1.eval()\n",
    "\n",
    "        ae2 = DeNoise2(layer_size=256).to(device)\n",
    "        ae2.load_state_dict(torch.load(os.path.join(model_directory, config['models'][1])))\n",
    "        ae2.eval()\n",
    "\n",
    "        ae3 = DeNoise2(layer_size=256).to(device)\n",
    "        ae3.load_state_dict(torch.load(os.path.join(model_directory, config['models'][2])))\n",
    "        ae3.eval()\n",
    "\n",
    "        ae4 = DeNoise2(layer_size=256).to(device)\n",
    "        ae4.load_state_dict(torch.load(os.path.join(model_directory, config['models'][3])))\n",
    "        ae4.eval()\n",
    "\n",
    "        results = {}\n",
    "\n",
    "        for method in config['methods']:\n",
    "            \n",
    "            results[method] = {}\n",
    "            for target in test_target_snr_dbs + ['all']:\n",
    "                results[method][target] = {\n",
    "                    'result': [],\n",
    "                    'snr_imp': [],\n",
    "                    'mse': [],\n",
    "                    'rmse': [],\n",
    "                    'mae': [],\n",
    "                    'psnr': [],\n",
    "                    'xcorr': [],\n",
    "                    'prd': [],\n",
    "                    'cpu_time': []\n",
    "                }\n",
    "            i = 0\n",
    "            for noise, clean in test_dataset:\n",
    "\n",
    "                cpu_time = 0\n",
    "                clean, noise, min_val, max_val = normalize(clean, noise)\n",
    "\n",
    "                if method == 'AE1':\n",
    "                    \n",
    "                    noise = noise.reshape(-1, 1, segment_size_samples).to(device)\n",
    "\n",
    "                    cpu_time_start = time.process_time()\n",
    "                    predicted = ae1(noise)\n",
    "                    cpu_time_stop = time.process_time()\n",
    "                    cpu_time = cpu_time_stop - cpu_time_start\n",
    "\n",
    "                    noise = noise.to('cpu').reshape(segment_size_samples).numpy()\n",
    "                    predicted = predicted.to('cpu').reshape(segment_size_samples).numpy()\n",
    "                elif method == 'AE2':\n",
    "                    noise = noise.reshape(-1, 1, segment_size_samples).to(device)\n",
    "\n",
    "                    cpu_time_start = time.process_time()\n",
    "                    predicted = ae2(noise)\n",
    "                    cpu_time_stop = time.process_time()\n",
    "                    cpu_time = cpu_time_stop - cpu_time_start\n",
    "\n",
    "                    noise = noise.to('cpu').reshape(segment_size_samples).numpy()\n",
    "                    predicted = predicted.to('cpu').reshape(segment_size_samples).numpy()\n",
    "                elif method == 'WD':\n",
    "                    noise = noise.numpy()\n",
    "\n",
    "                    cpu_time_start = time.process_time()\n",
    "                    predicted = denoise_wavelet(noise, method='BayesShrink', mode='soft', wavelet_levels=1, wavelet='sym8', rescale_sigma='True')\n",
    "                    cpu_time_stop = time.process_time()\n",
    "                    cpu_time = cpu_time_stop - cpu_time_start\n",
    "                elif method == 'AE3':\n",
    "                    # noise = noise.numpy()\n",
    "\n",
    "                    # cpu_time_start = time.process_time()\n",
    "                    # predicted = denoise_wavelet(noise, method='BayesShrink', mode='soft', wavelet_levels=1, wavelet='sym8', rescale_sigma='True')\n",
    "                    # noise = torch.from_numpy(predicted)\n",
    "                    # noise = noise.reshape(-1, 1, segment_size_samples).to(device)\n",
    "                    # predicted = ae1(noise)\n",
    "                    # cpu_time_stop = time.process_time()\n",
    "                    # cpu_time = cpu_time_stop - cpu_time_start\n",
    "\n",
    "                    # noise = noise.to('cpu').reshape(segment_size_samples).numpy()\n",
    "                    # predicted = predicted.to('cpu').reshape(segment_size_samples).numpy()\n",
    "\n",
    "                    noise = noise.reshape(-1, 1, segment_size_samples).to(device)\n",
    "\n",
    "                    cpu_time_start = time.process_time()\n",
    "                    predicted = ae3(noise)\n",
    "                    cpu_time_stop = time.process_time()\n",
    "                    cpu_time = cpu_time_stop - cpu_time_start\n",
    "\n",
    "                    noise = noise.to('cpu').reshape(segment_size_samples).numpy()\n",
    "                    predicted = predicted.to('cpu').reshape(segment_size_samples).numpy()\n",
    "                elif method == 'AE4':\n",
    "                    noise = noise.reshape(-1, 1, segment_size_samples).to(device)\n",
    "\n",
    "                    cpu_time_start = time.process_time()\n",
    "                    predicted = ae4(noise)\n",
    "                    cpu_time_stop = time.process_time()\n",
    "                    cpu_time = cpu_time_stop - cpu_time_start\n",
    "\n",
    "                    noise = noise.to('cpu').reshape(segment_size_samples).numpy()\n",
    "                    predicted = predicted.to('cpu').reshape(segment_size_samples).numpy()\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "                clean_old = clean.numpy()\n",
    "                clean, noise = denormalize(clean_old, noise, min_val, max_val)\n",
    "                _, predicted = denormalize(clean_old, predicted, min_val, max_val)\n",
    "\n",
    "                snr_data = snr(clean, noise)\n",
    "                for target in test_target_snr_dbs + ['all']:\n",
    "                    if (str(target) == 'all') or (snr_data > (target - 0.8) and snr_data < (target + 0.8)):\n",
    "                        i += 1\n",
    "                        results[method][target]['result'].append(predicted)\n",
    "                        results[method][target]['snr_imp'].append([snr(clean, predicted) - snr_data])\n",
    "                        results[method][target]['mse'].append([mse(clean, predicted)])\n",
    "                        results[method][target]['rmse'].append([rmse(clean, predicted)])\n",
    "                        results[method][target]['mae'].append([mae(clean, predicted)])\n",
    "                        # results[method][target]['psnr'].append([psnr(clean, predicted)])\n",
    "                        if config['mode'] == 1 and method == 'AE1':\n",
    "                            results[method][target]['xcorr'].append([xcorr(clean, predicted)])\n",
    "                        elif config['mode'] == 1 and method in ['AE1', 'AE3']:\n",
    "                            results[method][target]['xcorr'].append([xcorr(bandpass(clean), bandpass(predicted))])\n",
    "                        else:\n",
    "                            results[method][target]['xcorr'].append([xcorr(bandpass(clean), bandpass(predicted))])\n",
    "                        results[method][target]['prd'].append([prd(clean, predicted)])\n",
    "                        results[method][target]['cpu_time'].append(cpu_time)\n",
    "            \n",
    "            print(i)\n",
    "            value = 'all'\n",
    "            print('*** ' + method + ' ***')\n",
    "            print(len(results[method][value]['result']))\n",
    "            print('SNR IMP ' + str(np.mean(results[method][value]['snr_imp'])))\n",
    "            print('RMSE ' + str(np.mean(results[method][value]['rmse'])))\n",
    "            print('PRD ' + str(np.mean(results[method][value]['prd'])))\n",
    "            print('CPU TIME ' + str(np.mean(results[method][value]['cpu_time'])))\n",
    "            print()\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3266b49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing is done with a different set of target signal-to-noise ratios. Those are set here.\n",
    "train_dataset.dataset.add_noise(test_target_snr_dbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8fbe20c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n",
      "TEST\n"
     ]
    }
   ],
   "source": [
    "# The config describes, which parameters should be tested.\n",
    "config0 = {\n",
    "    'mode': 1,\n",
    "    'methods': ['AE1', 'AE2', 'AE3', 'AE4'], #, 'WD']# , 'AE1+WD']\n",
    "    'models': ['normal_bandpass/model_2_1024_relu_extended_10.pth', 'normal_full/model_2_1024_relu_extended_10_3.pth', 'normal_full/model_2_1024_relu_extended_10_3.pth', 'normal_full/model_2_1024_relu_extended_20.pth'],\n",
    "    'labels': ['On Training Data', 'On Results', 'On Training Data + Results', 'No Usage'],\n",
    "    'target_snr_print': [-1, 0, 1, 3, 8], # [-1, 0, 0.5, 1, 3, 5, 8]\n",
    "    'xlabel': 'Bandpass Filter Usage'\n",
    "}\n",
    "\n",
    "config1 = {\n",
    "    'mode': 0,\n",
    "    'methods': ['AE1', 'AE2', 'AE3', 'AE4'], #, 'WD']# , 'AE1+WD']\n",
    "    'models': ['normal_full/model_2_1024_relu_simple.pth', 'normal_full/model_2_1024_relu_extended_5.pth', 'normal_full/model_2_1024_relu_extended_10_3.pth', 'normal_full/model_2_1024_relu_extended_20.pth'],\n",
    "    'labels': ['1', '2', '3', '4'],\n",
    "    'target_snr_print': [-1, 0, 1, 3, 8], # [-1, 0, 0.5, 1, 3, 5, 8]\n",
    "    'xlabel': 'Noise Distribution'\n",
    "}\n",
    "\n",
    "config2 = {\n",
    "    'mode': 0,\n",
    "    'methods': ['AE1', 'AE2', 'WD'],\n",
    "    'models': ['normal_full/model_2_1024_relu_extended_10_3.pth', 'normal_full/model_1_1024_relu_extended_10.pth', 'normal_full/model_2_1024_relu_extended_10.pth', 'normal_full/model_2_1024_relu_extended_20.pth'],\n",
    "    'labels': ['AE1', 'AE2', 'WD'],\n",
    "    'target_snr_print': [-1, 0, 1, 3, 8], # [-1, 0, 0.5, 1, 3, 5, 8]\n",
    "    'xlabel': 'Denoising Method'\n",
    "}\n",
    "\n",
    "# 10 --> old, 10_3 --> newest\n",
    "results = test(config0)\n",
    "save_results(results, config0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3d34b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './models/loss_2_1024_relu_extended_10.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model1 \u001b[39m=\u001b[39m DeNoise2(layer_size\u001b[39m=\u001b[39m\u001b[39m256\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m----> 2\u001b[0m model1\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39;49mload(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(model_directory, \u001b[39m'\u001b[39;49m\u001b[39mloss_2_1024_relu_extended_10.pth\u001b[39;49m\u001b[39m'\u001b[39;49m)))\n\u001b[1;32m      4\u001b[0m model2 \u001b[39m=\u001b[39m DeNoise2(layer_size\u001b[39m=\u001b[39m\u001b[39m256\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      5\u001b[0m model2\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39mload(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(model_directory, \u001b[39m'\u001b[39m\u001b[39mloss_2_1024_relu_extended_5.pth\u001b[39m\u001b[39m'\u001b[39m)))\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/thesis-project-iJTh5vgr/lib/python3.10/site-packages/torch/serialization.py:771\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    768\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    769\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 771\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[1;32m    772\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    773\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    774\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    775\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    776\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/thesis-project-iJTh5vgr/lib/python3.10/site-packages/torch/serialization.py:270\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    269\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 270\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    271\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    272\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/thesis-project-iJTh5vgr/lib/python3.10/site-packages/torch/serialization.py:251\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[0;32m--> 251\u001b[0m     \u001b[39msuper\u001b[39m(_open_file, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './models/loss_2_1024_relu_extended_10.pth'"
     ]
    }
   ],
   "source": [
    "model1 = DeNoise2(layer_size=256).to(device)\n",
    "model1.load_state_dict(torch.load(os.path.join(model_directory, 'loss_2_1024_relu_extended_10.pth')))\n",
    "\n",
    "model2 = DeNoise2(layer_size=256).to(device)\n",
    "model2.load_state_dict(torch.load(os.path.join(model_directory, 'loss_2_1024_relu_extended_5.pth')))\n",
    "\n",
    "model1.eval()\n",
    "model2.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    i = random.choice(range(0, len(test_dataset)))\n",
    "    print(i)\n",
    "\n",
    "    noise = test_dataset[i][0]\n",
    "    clean = test_dataset[i][1]\n",
    "\n",
    "    clean, noise, min_val, max_val = normalize(clean, noise)\n",
    "    clean = clean.reshape(-1, 1, segment_size_samples).to(device)\n",
    "    noise = noise.reshape(-1, 1, segment_size_samples).to(device)\n",
    "\n",
    "    predicted = model1(noise).to('cpu').reshape(segment_size_samples).numpy()\n",
    "    predicted2 = model2(noise).to('cpu').reshape(segment_size_samples).numpy()\n",
    "    clean_old = clean.to('cpu').reshape(segment_size_samples).numpy()\n",
    "    noise = noise.to('cpu').reshape(segment_size_samples).numpy()\n",
    "    \n",
    "    clean, noise = denormalize(clean_old, noise, min_val, max_val)\n",
    "    _, predicted = denormalize(clean_old, predicted, min_val, max_val)\n",
    "    _, predicted2 = denormalize(clean_old, predicted2, min_val, max_val)\n",
    "    # predicted2 = denoise_wavelet(noise, method='BayesShrink', mode='soft', wavelet_levels=1, wavelet='sym8', rescale_sigma='True')\n",
    "\n",
    "    from_val = 0\n",
    "    to_val = 1024\n",
    "\n",
    "    print(mse(clean, noise))\n",
    "    print(mse(clean, predicted))\n",
    "    print()\n",
    "\n",
    "    print(mae(clean, noise))\n",
    "    print(mae(clean, predicted))\n",
    "    print()\n",
    "\n",
    "    print(snr(clean, noise))\n",
    "    print(snr(clean, predicted))\n",
    "    print(snr(clean, predicted2))\n",
    "    print()\n",
    "\n",
    "    print(psnr(clean, noise))\n",
    "    print(psnr(clean, predicted))\n",
    "    print()\n",
    "\n",
    "    print(xcorr(clean, noise))\n",
    "    print(xcorr(clean, predicted))\n",
    "    print()\n",
    "    \n",
    "    time_from = 0\n",
    "    time_to = 1024\n",
    "\n",
    "    plt.plot(range(time_to)[time_from:time_to], bandpass(noise)[time_from:time_to])\n",
    "    plt.show()\n",
    "    plt.plot(range(time_to)[time_from:time_to], bandpass(clean)[time_from:time_to])\n",
    "    plt.show()\n",
    "    plt.plot(range(time_to)[time_from:time_to], bandpass(predicted)[time_from:time_to])\n",
    "    plt.show()\n",
    "    plt.plot(range(time_to)[time_from:time_to], bandpass(predicted2)[time_from:time_to])\n",
    "    plt.show()\n",
    "\n",
    "    torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('thesis-project-iJTh5vgr')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "5358558d97acbe0e10b27ba4ce90186e9cdab3f70950976f7d22547e5a6890e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
